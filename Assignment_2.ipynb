{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luka140/DL/blob/main/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssLI7a7GKzgB"
      },
      "source": [
        "# CS4240 Deep Learning - Assignment 2\n",
        "\n",
        "*These lab assignments are new in the CS4240 Deep Learning course. We'd like to hear what you think!*\n",
        "\n",
        "*Please post any feedback you have on Brightspace. Thanks!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL148V_8rxq5"
      },
      "source": [
        "To start working on the assignment in Colab, save a copy on your Google Drive (`File` $\\rightarrow$ `Save a copy in Drive`).\n",
        "\n",
        "To work on the assignments locally, configure your conda environment (see instructions on Brightspace) and download this assignment as an IPython Notebook (`File` $\\rightarrow$ `Download .ipynb`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdx7o5N2S2Tn"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this assignment you will be introduced to the backpropagation algorithm and you will train your first neural network. We will continue from the layers implemented in the previous assignment and define the backward passes to compute gradients. Finally, you will use your neural network to solve the XOR problem from the previous assignment, as well as to classify handwritten digits from the MNIST dataset.\n",
        "\n",
        "**Prerequisites:**\n",
        "* Completion of previous assignment.\n",
        "* Basic knowledge of Python and Numpy. Recommended tutorial for Python and Numpy [here](https://cs231n.github.io/python-numpy-tutorial/).\n",
        "* We recommend you to have a look at [this excellent tutorial on PyTorch Tensors](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py).\n",
        "\n",
        "**Learning objectives:**\n",
        "* Understanding, deriving and implementing the backward pass for a fully connected layer and the ReLU and Sigmoid non-linear activation functions in PyTorch;\n",
        "* Understanding and implementing the backpropagation algorithm and the training loop;\n",
        "* Understanding Softmax and the cross-entropy loss.\n",
        "\n",
        "We will share the solutions one week after the assignments are published. Throughout the assignment you will validate your code by comparing the outputs of your own implementations to the equivalent PyTorch implementations.\n",
        "\n",
        "**For your own implementations you may only use basic tensor operations from the `torch` module: no `torch.nn`, `torch.nn.F` or others.**\n",
        "\n",
        "---\n",
        "\n",
        "When answering coding questions make sure to write your own code within the designated part of the code block as illustrated here:\n",
        "```python\n",
        "#############################################################################\n",
        "#                       TODO: Implement function x                          #\n",
        "#############################################################################\n",
        "# Replace \"pass\" statement with your code\n",
        "pass\n",
        "#############################################################################\n",
        "#                            END OF YOUR CODE                               #\n",
        "#############################################################################\n",
        "```\n",
        "\n",
        "Please pay attention to the question boxes and try to think about them. The boxes are indicated as follows:\n",
        "\n",
        "****\n",
        "**Questions?**\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIvAOzFjhtw6"
      },
      "source": [
        "# Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuIHuF2dMzSV"
      },
      "source": [
        "## A2.1 Linear layer - backward pass\n",
        "\n",
        "In A2.1 we will build upon the linear layer from A1.1 and implement the backward pass. We then perform a forward pass and backward pass on some dummy input and compare the gradients to the Pytorch implementation of a Linear layer.\n",
        "\n",
        "**Backpropagation**\n",
        "\n",
        "Backpropagation is an efficient algorithm to modularly calculate the gradients of the loss $\\mathcal{L}$ with respect to the trainable parameters in our network. The gradient is used in (Stochastic) Gradient Descent to update the trainable parameters to minimize the loss. For now we only consider how to calculate the gradients in the `backward` function of a layer, later on we will also implement the update step.\n",
        "\n",
        "The forward pass of a linear layer is defined as $y=xw+b$, where $w$ and $b$ are the trainable weight and bias parameters, respectively. The gradients w.r.t. $w$ and $b$ are then calculated using the chain rule as\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\frac{\\partial y}{\\partial w}, \\qquad \\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\frac{\\partial y}{\\partial b}.$$\n",
        "\n",
        "Here, $\\frac{\\partial \\mathcal{L}}{\\partial y}$ is the *upstream gradient*, i.e. the gradient flowing from the deeper layer into the current layer. The upstream gradient will be the input variable in our `backward` function.\n",
        "The other two terms are the *local gradients* and will be computed inside the `backward` function as\n",
        "$$\\frac{\\partial y}{\\partial w}=x, \\qquad \\frac{\\partial y}{\\partial b}=1.$$\n",
        "\n",
        "As you can see, the input of the layer $x$ is used for computing the gradient in the backward pass. We therefore need to store this value during the forward pass as a cache variable so that we don't have to compute it again.\n",
        "\n",
        "Furthermore, since the shallower layer also requires an upstream gradient, our `backward` future needs to return a *downstream gradient*. The downstream gradient of a layer is simply its upstream gradient times the local gradient of its ouput with respect to its input: $\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\frac{\\partial y}{\\partial x}$. For a linear layer $\\frac{\\partial y}{\\partial x}=w$ and so $\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial y} w$.\n",
        "\n",
        "Let us further clarify using an example with a two-layer neural network. The network output is given by\n",
        "$$y_2 = y_1 w_2 + b_2 = (x_1 w_1 + b_1) w_2 + b_2.$$\n",
        "The gradients for the first layer are calculated using the chain rule:\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial w_1} = \\frac{\\partial \\mathcal{L}}{\\partial y_2} \\frac{\\partial y_2}{\\partial y_1} \\frac{\\partial y_1}{\\partial w_1} = \\frac{\\partial \\mathcal{L}}{\\partial y_1} \\frac{\\partial y_1}{\\partial w_1}.$$\n",
        "\n",
        "From the perspective of layer 2, $\\frac{\\partial \\mathcal{L}}{\\partial y_2}$ is the upstream gradient, $\\frac{\\partial y_2}{\\partial y_1} = \\frac{\\partial y_2}{\\partial x_2} = w_2$ is the local gradient, and $\\frac{\\partial \\mathcal{L}}{\\partial y_2} \\frac{\\partial y_2}{\\partial y_1} = \\frac{\\partial \\mathcal{L}}{\\partial y_1}$ is the downstream gradient. From the perspective of layer 1, $\\frac{\\partial \\mathcal{L}}{\\partial y_1}$ is the upstream gradient.\n",
        "\n",
        "You will now implement the `backward` function in the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWcqUhVgfR5A"
      },
      "source": [
        "class Linear(object):\n",
        "    \"\"\"\n",
        "    Fully connected layer.\n",
        "\n",
        "    Args:\n",
        "        in_features: number of input features\n",
        "        out_features: number of output features\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        # Define placeholder tensors for layer weight and bias. The placeholder\n",
        "        # tensors should have the correct dimension according to the in_features\n",
        "        # and out_features variables.\n",
        "        self.weight = torch.Tensor(in_features, out_features)\n",
        "        self.bias = torch.Tensor(out_features)\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.init_params()\n",
        "\n",
        "        # NEW: Define a cache varible to save computation, because some of the\n",
        "        # forward pass values would be used during backward pass.\n",
        "        self.cache = None\n",
        "\n",
        "        # NEW: Define variables to store the gradients of the weight and bias\n",
        "        # calculated during the backward pass\n",
        "        self.weight_grad = None\n",
        "        self.bias_grad = None\n",
        "\n",
        "    def init_params(self, std=1.):\n",
        "        \"\"\"\n",
        "        Initialize layer parameters. Sample weight from Gaussian distribution\n",
        "        and bias uniform distribution.\n",
        "\n",
        "        Args:\n",
        "            std: Standard deviation of Gaussian distribution (default: 1.0)\n",
        "        \"\"\"\n",
        "\n",
        "        self.weight = std*torch.randn_like(self.weight)\n",
        "        self.bias = torch.rand_like(self.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of linear layer: multiply input tensor by weights and add\n",
        "        bias. Store input tensor as cache variable.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor\n",
        "\n",
        "        Returns:\n",
        "            y: output tensor\n",
        "        \"\"\"\n",
        "\n",
        "        y = torch.mm(x, self.weight) + self.bias  # forward pass\n",
        "\n",
        "        ########################################################################\n",
        "        #                TODO: Store input as cache variable                   #\n",
        "        ########################################################################\n",
        "\n",
        "        self.cache = x\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return y\n",
        "\n",
        "    def backward(self, dupstream):\n",
        "        \"\"\"\n",
        "        Backward pass of linear layer: calculate gradients of loss with respect\n",
        "        to weight and bias and return downstream gradient dx.\n",
        "\n",
        "        Args:\n",
        "            dupstream: Gradient of loss with respect to output of this layer.\n",
        "\n",
        "        Returns:\n",
        "            dx: Gradient of loss with respect to input of this layer.\n",
        "        \"\"\"\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "        # print(\"w\", self.weight)\n",
        "        # print('dup', dupstream)\n",
        "        dx = dupstream @ self.weight.T\n",
        "\n",
        "        self.weight_grad = self.cache.T @ dupstream\n",
        "        self.bias_grad = dupstream\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ##############w##########################################################\n",
        "\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtE7ytKSFBst"
      },
      "source": [
        "Now test the forward and backward pass of the layer on some dummy input.\n",
        "\n",
        "****\n",
        "**What will be the shape of gradient of x w.r.t. y?**\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSALY1wPFGXZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019c2b02-b8f6-4928-a9a0-24fc56d5f462"
      },
      "source": [
        "# Define layer dimensions\n",
        "n_samples, in_features, out_features = 2, 3, 4\n",
        "# Make random input tensor of dimensions [n_samples, in_features]\n",
        "x = torch.randn((n_samples, in_features))\n",
        "# Define upstream gradient dL/dy as randn\n",
        "dy = torch.randn((n_samples, out_features))\n",
        "\n",
        "# Create a layer from the Linear object class above\n",
        "layer = Linear(in_features, out_features)\n",
        "# Forward pass\n",
        "y = layer.forward(x)\n",
        "# Backward pass\n",
        "dx = layer.backward(dy)\n",
        "\n",
        "# What will be the shape of gradient of x w.r.t. y?\n",
        "print('Shape of gradient x is:', dx.shape)\n",
        "print('Shape correct: ', dx.shape == x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of gradient x is: torch.Size([2, 3])\n",
            "Shape correct:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L5vbHyFIE9m"
      },
      "source": [
        "If the above code block has not returned any errors we can compare our implementation to the PyTorch linear layer [[docs](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)] from `torch.nn`. We do so by initializing a `nn.Linear` layer and setting the `weight` and `bias` to the same values as in our own linear layer.\n",
        "\n",
        "PyTorch layers store their parameters as a `Parameter`, which is a `Tensor` subclass with some special properties [[docs](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html)]. We therefore need to wrap our `layer.weight` and `layer.bias` in a `nn.Parameter` when using in `nn.Linear`. Moreover, the `weight` tensor is transposed in `nn.Linear`.\n",
        "\n",
        "PyTorch does not require the gradients to be specified for each layer individually but instead uses a mechanism called Autograd for automatic differentiation. To enable this for our dummy input we need to set the `requires_grad` parameter of the input tensor to `True`. More information about Autograd can be found in the official [docs](https://pytorch.org/docs/stable/notes/autograd.html) and in this excellent [tutorial](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa-_W-MgFJzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ce7e6fb-4a85-46b8-927c-4fac66610718"
      },
      "source": [
        "# Create Linear layer from torch.nn module\n",
        "torch_layer = nn.Linear(in_features, out_features)\n",
        "\n",
        "# Load the parameters from our layer into the Pytorch layer\n",
        "torch_layer.weight = nn.Parameter(layer.weight.T) # transpose weight by .t()\n",
        "torch_layer.bias = nn.Parameter(layer.bias, requires_grad=True)\n",
        "\n",
        "# We create a copy of the input tensor so we can use x for all other layers\n",
        "# without overwriting its gradients\n",
        "x_lin = x.clone()\n",
        "# Enable requires_grad for x_lin\n",
        "x_lin.requires_grad = True\n",
        "\n",
        "# Perform forward pass\n",
        "torch_y = torch_layer(x_lin)\n",
        "\n",
        "# Perform bacward pass\n",
        "torch_y.backward(dy)\n",
        "\n",
        "# What will be the shape of gradient of x w.r.t. y?\n",
        "print('Shape of gradient x is:', dx.shape)\n",
        "print('Shape correct:', x_lin.grad.shape == x_lin.shape)\n",
        "print('bias grad:', torch_layer.bias.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of gradient x is: torch.Size([2, 3])\n",
            "Shape correct: True\n",
            "bias grad: tensor([-0.2277, -0.0750, -0.7622, -1.5334])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjE7DIp9Ltut"
      },
      "source": [
        "We now compare the gradients of both implementations using `torch.allclose` [[docs](https://pytorch.org/docs/stable/generated/torch.allclose.html)], which returns `True` if all elements in both tensors are sufficiently \"close\" to each other.\n",
        "\n",
        "Your backward implementation of the linear layer is therefore correct if `True` is returned for all gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DMC0nrYNkIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42abb7dd-5ae0-4726-e033-62aeafb58c86"
      },
      "source": [
        "# Compare gradients of x, weight, bias w.r.t. y using torch.allclose\n",
        "dx_same = torch.allclose(dx, x_lin.grad)\n",
        "print('dx identical: ', dx_same)\n",
        "dw_same = torch.allclose(layer.weight_grad, torch_layer.weight.grad.T)\n",
        "print('dw identical: ', dw_same)\n",
        "db_same = torch.allclose(layer.bias_grad, torch_layer.bias.grad)\n",
        "print('db identical: ', db_same)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dx identical:  True\n",
            "dw identical:  True\n",
            "db identical:  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N41y5ntsMw40"
      },
      "source": [
        "## A2.2 Non-linear activation functions - backward pass\n",
        "\n",
        "Similarly we implement the backward passes for both non-linearities. The non-linearities are defined as follows:\n",
        "\n",
        "$$\\text{ReLU}(x)=\\max(0,x)\\\\\n",
        "\\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1+\\exp(-x)}$$\n",
        "\n",
        "The derivations of the derivatives are left for you as an exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOphZSwhMqn6"
      },
      "source": [
        "class ReLU(object):\n",
        "    \"\"\"\n",
        "    ReLU non-linear activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ReLU, self).__init__()\n",
        "\n",
        "        # NEW: Define a cache variable because some of the forward pass values\n",
        "        # would be used during backward pass.\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of ReLU non-linear activation function: y=max(0,x). Store\n",
        "        input tensor as cache variable.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor\n",
        "\n",
        "        Returns:\n",
        "            y: output tensor\n",
        "        \"\"\"\n",
        "\n",
        "        y = torch.clamp(x, min=0)  # forward pass\n",
        "\n",
        "        ########################################################################\n",
        "        #                   TODO: Update cache variable.                       #\n",
        "        ########################################################################\n",
        "\n",
        "        self.cache = x\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return y\n",
        "\n",
        "    def backward(self, dupstream):\n",
        "        \"\"\"\n",
        "        Backward pass of ReLU non-linear activation function: return downstream\n",
        "        gradient dx.\n",
        "\n",
        "        Args:\n",
        "            dupstream: Gradient of loss with respect to output of this layer.\n",
        "\n",
        "        Returns:\n",
        "            dx: Gradient of loss with respect to input of this layer.\n",
        "        \"\"\"\n",
        "\n",
        "        # Making sure that we don't modify the incoming upstream gradient\n",
        "        dupstream = dupstream.clone()\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        if self.cache <= 0:\n",
        "          dx = 0\n",
        "        else:\n",
        "          dx = dupstream\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return dx\n",
        "\n",
        "class Sigmoid(object):\n",
        "    \"\"\"\n",
        "    Sigmoid non-linear activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Sigmoid, self).__init__()\n",
        "\n",
        "        # NEW: Define a cache variable because some of the forward pass value\n",
        "        # would be used during backward pass.\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of Sigmoid non-linear activation function: y=1/(1+exp(-x)).\n",
        "        Store input tensor as cache variable.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor\n",
        "\n",
        "        Returns:\n",
        "            y: output tensor\n",
        "        \"\"\"\n",
        "\n",
        "        y = 1.0 / (1.0 + torch.exp(-x))\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        self.cache = x\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return y\n",
        "\n",
        "    def backward(self, dupstream):\n",
        "        \"\"\"\n",
        "        Backward pass of Sigmoid non-linear activation function: return\n",
        "        downstream gradient dx.\n",
        "\n",
        "        Args:\n",
        "            dupstream: Gradient of loss with respect to output of this layer.\n",
        "\n",
        "        Returns:\n",
        "            dx: Gradient of loss with respect to input of this layer.\n",
        "        \"\"\"\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        pass\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtOu5BCKQvky"
      },
      "source": [
        "Test the backward pass of both non-linearities using the dummy input and gradients from before.\n",
        "\n",
        "****\n",
        "**What will be the shapes of the output tensors?**\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-sCWIhNOetw"
      },
      "source": [
        "# Define layer dimensions and dummy input\n",
        "n_samples, in_features = 2, 3\n",
        "# Make random input tensor of dimensions [n_samples, in_features]\n",
        "x = torch.randn((n_samples, in_features))\n",
        "\n",
        "# Define ReLU and Sigmoid layers\n",
        "relu = ReLU()\n",
        "sigmoid = Sigmoid()\n",
        "\n",
        "# Define upstream gradient dL/dy as ones\n",
        "dy = torch.randn((n_samples, in_features))\n",
        "print('dy:', dy)\n",
        "\n",
        "########################################################################\n",
        "#    TODO: Perform a forward and backward pass with the ReLU and       #\n",
        "#                          Sigmoid functions                           #\n",
        "########################################################################\n",
        "\n",
        "pass\n",
        "\n",
        "########################################################################\n",
        "#                         END OF YOUR CODE                             #\n",
        "########################################################################\n",
        "\n",
        "# What will be the shapes of gradient tensors dx_relu and dx_sigmoid?\n",
        "print('Output shapes from ReLU and Sigmoid: ', dx_relu.shape, dx_sigmoid.shape)\n",
        "print('Shape of gradient x from ReLU correct:', dx_relu.shape == x.shape)\n",
        "print('Shape of gradient x from Sigmoid correct:', dx_sigmoid.shape == x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hiL-1zwRh_G"
      },
      "source": [
        "Now perform a forward and backward pass with the ReLU and Sigmoid activation functions from `torch.nn` and compare the outputs to your implementation.\n",
        "\n",
        "A list of all available non-linearities in PyTorch can be found [[here](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCpv6e_bR6Dg"
      },
      "source": [
        "# We create a copy of the input tensor so we can use x for all other layers\n",
        "# without overwriting its gradients\n",
        "x_relu = x.clone()\n",
        "x_sigmoid = x.clone()\n",
        "# Enable requires_grad\n",
        "x_relu.requires_grad = True\n",
        "x_sigmoid.requires_grad = True\n",
        "\n",
        "print('dy: ', dy)\n",
        "print('---')\n",
        "\n",
        "# ReLU forward pass\n",
        "torch_relu = nn.ReLU()\n",
        "torch_y_relu = torch_relu(x_relu)\n",
        "torch_y_relu.backward(dy)\n",
        "\n",
        "# Sigmoid forward pass\n",
        "torch_sigmoid = nn.Sigmoid()\n",
        "torch_y_sigmoid = torch_sigmoid(x_sigmoid)\n",
        "torch_y_sigmoid.backward(dy)\n",
        "\n",
        "# Compare outputs using torch.allclose\n",
        "dx_relu_same = torch.allclose(dx_relu, x_relu.grad)\n",
        "print('dx_relu identical: ', dx_relu_same)\n",
        "\n",
        "print(f\"dx_relu:\\n {dx_relu}\")\n",
        "print(f\"pytorch x_relu.grad:\\n {x_relu.grad}\")\n",
        "\n",
        "print('---')\n",
        "\n",
        "dx_sigmoid_same = torch.allclose(dx_sigmoid, x_sigmoid.grad)\n",
        "print('dx_sigmoid identical: ', dx_sigmoid_same)\n",
        "\n",
        "print(f\"dx_sigmoid:\\n {dx_sigmoid}\")\n",
        "print(f\"pytorch x_sigmoid.grad:\\n {x_sigmoid.grad}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7bn6PyoNF8r"
      },
      "source": [
        "## A2.3 Network class - backward pass and update step\n",
        "\n",
        "We will now extend our neural network class with a `backward` function that calculates the gradients for all parameters in the network and a `optimizer_step` function that performs a parameter update step using gradient descent. After calculating the gradients the weights and biases are updated as\n",
        "$$w' = w - \\eta\\frac{\\partial L}{\\partial w}, \\qquad b' = b - \\eta\\frac{\\partial L}{\\partial b},$$\n",
        "where $w'$ and $b'$ are the new weights and biases and $\\eta$ is the gradient descent step size, also called the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi8luwCMgobr"
      },
      "source": [
        "class Net(object):\n",
        "    \"\"\"\n",
        "    Neural network object containing layers.\n",
        "\n",
        "    Args:\n",
        "        layers: list of layers in neural network\n",
        "    \"\"\"\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "\n",
        "        # Initialize params\n",
        "        self.reset_params()\n",
        "\n",
        "    def reset_params(self, std=1.):\n",
        "        \"\"\"\n",
        "        Reset network parameters. Applies `init_params` to all layers with\n",
        "        learnable parameters.\n",
        "\n",
        "        Args:\n",
        "            std: Standard deviation of Gaussian distribution (default: 0.1)\n",
        "        \"\"\"\n",
        "\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'init_params'):\n",
        "                layer.init_params(std=std)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs forward pass through all layers of the network.\n",
        "\n",
        "        Args:\n",
        "            x: input tensor\n",
        "\n",
        "        Returns:\n",
        "            x: output tensor\n",
        "        \"\"\"\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def backward(self, dupstream):\n",
        "        \"\"\"\n",
        "        Performs backward pass through all layers of the network.\n",
        "\n",
        "        Args:\n",
        "            dupstream: Gradient of loss with respect to output.\n",
        "        \"\"\"\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        ########################################################################\n",
        "\n",
        "        pass\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################\n",
        "\n",
        "        return dx\n",
        "\n",
        "    def optimizer_step(self, lr):\n",
        "        \"\"\"\n",
        "        Updates network weights by performing a step in the negative gradient\n",
        "        direction in each layer. The step size is determined by the learning\n",
        "        rate.\n",
        "\n",
        "        Args:\n",
        "            lr: Learning rate to use for update step.\n",
        "        \"\"\"\n",
        "\n",
        "        ########################################################################\n",
        "        #                  TODO: Implement this function                       #\n",
        "        #    Hint: use `if hasattr(layer, 'weight')` to check if a layer has   #\n",
        "        #                       trainable parameters.                          #\n",
        "        ########################################################################\n",
        "\n",
        "        pass\n",
        "\n",
        "        ########################################################################\n",
        "        #                         END OF YOUR CODE                             #\n",
        "        ########################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xLhaGmqNdAw"
      },
      "source": [
        "We will create a simple 2-layer network with ReLU non-linearity and test the forward and backward pass using the same dummy input as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k4uPheAjLsY"
      },
      "source": [
        "# Define layer dimensions\n",
        "n_samples, in_features, hidden_dim, out_features = 2, 3, 5, 4\n",
        "# Make random input tensor of dimensions [n_samples, in_features]\n",
        "x = torch.randn((n_samples, in_features))\n",
        "\n",
        "\n",
        "# Define and initialize layers\n",
        "layers = [Linear(in_features, hidden_dim),\n",
        "          ReLU(),\n",
        "          Linear(hidden_dim, out_features)]\n",
        "\n",
        "# Initialize network\n",
        "net = Net(layers)\n",
        "\n",
        "# Do forward pass\n",
        "y = net.forward(x)\n",
        "\n",
        "# Gradient of y w.r.t. y is 1\n",
        "dy = torch.randn((n_samples, out_features))\n",
        "\n",
        "# Do backward pass\n",
        "dx = net.backward(dy)\n",
        "\n",
        "# What will be the shape of gradient x?\n",
        "print('Shape of gradient x:', dx.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LMryIa6V7Zd"
      },
      "source": [
        "We will now again create the same neural network in PyTorch. PyTorch uses the Autograd mechanism to calculate all gradients in the network, so there is no need to define a backward function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mln6iWyrP1NF"
      },
      "source": [
        "class TorchNet(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch neural network. Network layers are defined in __init__ and forward\n",
        "    pass implemented in forward.\n",
        "\n",
        "    Args:\n",
        "        in_features: number of features in input layer\n",
        "        hidden_dim: number of features in hidden dimension\n",
        "        out_features: number of features in output layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, hidden_dim, out_features):\n",
        "        super(TorchNet, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(in_features, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_dim, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize Pytorch network\n",
        "torch_net = TorchNet(in_features, hidden_dim, out_features)\n",
        "print(torch_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQAFYgOtXTFX"
      },
      "source": [
        "We will now again compare the gradients of the loss w.r.t. to the input tensor for both networks. Therefore we again need to load the weights from our network into the PyTorch network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBYiJnRwQXqB"
      },
      "source": [
        "# Load the parameters from our model into the Pytorch model\n",
        "torch_net.layer1.weight = nn.Parameter(net.layers[0].weight.T) # transpose weight by .T\n",
        "torch_net.layer1.bias = nn.Parameter(net.layers[0].bias)\n",
        "torch_net.layer2.weight = nn.Parameter(net.layers[2].weight.T) # transpose weight by .T\n",
        "torch_net.layer2.bias = nn.Parameter(net.layers[2].bias)\n",
        "\n",
        "# Make copy of x\n",
        "torch_x = x.clone()\n",
        "torch_x.requires_grad = True\n",
        "\n",
        "# Perform forward pass\n",
        "torch_y = torch_net(torch_x)\n",
        "\n",
        "# Perform backward pass\n",
        "torch_y.backward(dy)\n",
        "\n",
        "# What will be the shape of gradient x?\n",
        "print('Shape of gradient x is correct:', dx.shape == torch_x.grad.shape)\n",
        "\n",
        "# Compare gradients using torch.allclose\n",
        "dx_same = torch.allclose(dx, torch_x.grad)\n",
        "print('Gradients identical: ', dx_same)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKrDW462NvNQ"
      },
      "source": [
        "##A2.4 Revisiting the XOR problem\n",
        "\n",
        "We revisit the XOR problem for the previous assignment and solve it by training our network with the backpropagation algorithm.\n",
        "\n",
        "The XOR problem consists of 4 data points belonging to 2 classes which cannot be separated by a linear decision boundary.\n",
        "\n",
        "| x0   | x1   | y    |\n",
        "| ---- | ---- | ---- |\n",
        "| 0    | 0    | 0    |\n",
        "| 0    | 1    | 1    |\n",
        "| 1    | 0    | 1    |\n",
        "| 1    | 1    | 0    |\n",
        "\n",
        "The class labels `y` are [one-hot encoded](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics), i.e. for each class a binary value indicates whether the sample belongs to it or not. For instance, given classes `1,2,3,4,5` the one-hot encoding of class `4` is given by `[0,0,0,1,0]`. One-hot encoding is a natural way to represent class labels in a classification task since a neural network outputs a class probability vector. `[0,0,0,1,0]` then simply corresponds to a 0% chance of the sample belonging the classes `1,2,3,5` and a 100% chance of it belonging to class `4`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR9D37H6JytZ"
      },
      "source": [
        "x_xor = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
        "y_xor = torch.tensor([[1, 0], [0, 1], [0, 1], [1, 0]]) # one-hot encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll4xMtgPKpSc"
      },
      "source": [
        "def plot(x, y, net=None):\n",
        "    \"\"\"\n",
        "    Plotter function for XOR dataset and classifier boundaries (optional).\n",
        "\n",
        "    Args:\n",
        "        x: Nx2 dimensional data\n",
        "        y: N dimensional labels\n",
        "        net: Model which has a forward function\n",
        "    \"\"\"\n",
        "    # Convert one-hot to class id\n",
        "    y = torch.argmax(y, dim=1)\n",
        "\n",
        "    # Plot decision boundary if net is given\n",
        "    if net:\n",
        "        h = 0.005\n",
        "        x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
        "        y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
        "\n",
        "        xx, yy = torch.meshgrid(torch.arange(x_min, x_max, h),\n",
        "                                torch.arange(y_min, y_max, h))\n",
        "\n",
        "        in_tensor = torch.cat((xx.reshape((-1,1)), yy.reshape((-1,1))), dim=1)\n",
        "\n",
        "        z = net.forward(in_tensor)\n",
        "        z = torch.argmax(z, dim=1)\n",
        "        z = z.reshape(xx.shape)\n",
        "        plt.contourf(xx, yy, z, cmap=plt.cm.coolwarm)\n",
        "\n",
        "    # Plot data points\n",
        "    plt.scatter(x[:, 0], x[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    plt.title('XOR problem')\n",
        "    plt.xlabel('x0')\n",
        "    plt.ylabel('x1')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Visualize\n",
        "plot(x_xor, y_xor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1PAklq65pew"
      },
      "source": [
        "## A2.5 Revisiting the training loop\n",
        "\n",
        "We will now revisit the training loop and implement the backward pass and optimizer step to train the network from the training data. We therefore need to implement the gradient of the MSE loss with respect to the prediction $\\hat{y}$.\n",
        "\n",
        "Given prediction $\\hat{y}$ and groud-truth $y$ the MSE loss is defined as:\n",
        "$$\\text{MSE}(\\hat{y},y)= \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}-y)^2 $$\n",
        "\n",
        "Again keep in mind that $\\hat{y}$ is a class probability vector and $y$ is a one-hot encoded representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLZE2-Pe0zkq"
      },
      "source": [
        "def MSELoss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes mean squared error loss.\n",
        "\n",
        "    Args:\n",
        "      y_true: Tensor containing true labels.\n",
        "      y_pred: Tensor containing predictions.\n",
        "\n",
        "    Return:\n",
        "      loss: Mean squared error loss.\n",
        "      grad: Gradient of loss w.r.t. y_pred.\n",
        "    \"\"\"\n",
        "    # Calculate mean squared error between y_true and y_pred\n",
        "    loss = torch.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "    ########################################################################\n",
        "    #             TODO: Implement the gradient dL/dy_hat                   #\n",
        "    ########################################################################\n",
        "\n",
        "    pass\n",
        "\n",
        "    ########################################################################\n",
        "    #                          END OF YOUR CODE                            #\n",
        "    ########################################################################\n",
        "\n",
        "    return loss, grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ8jint2Kbqk"
      },
      "source": [
        "You will now implement the training loop, which roughly speaking consists of the following steps:\n",
        "```\n",
        "while not converged:\n",
        " 1. feed training sample to network to predict output (forward step)\n",
        " 2. compare prediction to label (compute loss)\n",
        " 3. use comparison to update network parameters (backward and optimizer step)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--1sxOkIQzA4"
      },
      "source": [
        "# Define network dimensions\n",
        "in_features, hidden_dim, out_features = 2, 10, 2\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 1e-2  # step size for gradient descent\n",
        "optim_steps = 100  # network should generally converge within 100 steps\n",
        "\n",
        "# Define and initialize layers\n",
        "layers = [Linear(in_features, hidden_dim),\n",
        "          ReLU(),\n",
        "          Linear(hidden_dim, out_features)]\n",
        "\n",
        "# Initialize network\n",
        "net = Net(layers)\n",
        "\n",
        "# Define list to store loss and accuracy of each iteration\n",
        "losses = []\n",
        "accs = []\n",
        "\n",
        "for i in range(optim_steps):\n",
        "    # Perform forward pass with x_xor as input and y_pred as output variables\n",
        "    y_pred = net.forward(x_xor)\n",
        "\n",
        "    ########################################################################\n",
        "    #    TODO: Calculate MSE loss between prediction and labels (y_xor)    #\n",
        "    #                         and append to list.                          #\n",
        "    ########################################################################\n",
        "\n",
        "    pass\n",
        "\n",
        "    ########################################################################\n",
        "    #                          END OF YOUR CODE                            #\n",
        "    ########################################################################\n",
        "\n",
        "    ########################################################################\n",
        "    #            TODO: Perform backward pass and optimizer step.           #\n",
        "    ########################################################################\n",
        "\n",
        "    pass\n",
        "\n",
        "    ########################################################################\n",
        "    #                          END OF YOUR CODE                            #\n",
        "    ########################################################################\n",
        "\n",
        "    # Calculate accuracy of prediction\n",
        "    correct = torch.argmax(y_pred, axis=1) == torch.argmax(y_xor, axis=1)\n",
        "    accs.append(torch.sum(correct)/len(y_pred))\n",
        "\n",
        "# Print prediction\n",
        "print(y_pred)\n",
        "\n",
        "# Plot loss and accuracy\n",
        "plt.figure(figsize=(14,4))\n",
        "plt.subplot(1,3,1)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(losses)\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.plot(accs)\n",
        "plt.grid()\n",
        "\n",
        "# Show decision boundary\n",
        "plt.subplot(1,3,3)\n",
        "plot(x_xor, y_xor, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyitaiDVfc4Q"
      },
      "source": [
        "The network should now successfully converge within a reasonable number of iterations and solve the XOR problem with 100% accuracy. You trained your first neural network, great job! This is still a relatively primitive optimization algorithm and we will dive more in depth into optimizing neural networks in later assignments.\n",
        "\n",
        "Replace the ReLU non-linearity by the Sigmoid function and train the network again. You will see that the network has much more difficulty converging - in fact, you might need to increase the number of `optim_steps` to `500`. If you look at the loss graph you'll see that the network initially learns quickly, i.e. the loss rapidly decreases, but learning slows down far before reaching 100% accuracy. Since the network parameters are updated by gradient descent, slow learning simply means that the gradients have become small. To understand why this happens, let's again look at the Sigmoid function $\\sigma(x)$ and its derivative $\\sigma'(x)$:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSCQHbk0Sy7L"
      },
      "source": [
        "# Plot the sigmoid function and its derivative\n",
        "x = torch.linspace(-5, 5, 51)\n",
        "y = 1 / (1+torch.exp(-x))\n",
        "dydx = torch.exp(x)/(1+torch.exp(x))**2\n",
        "\n",
        "plt.title('Sigmoid function and derivative')\n",
        "plt.plot(x,y)\n",
        "plt.plot(x,dydx)\n",
        "plt.legend([r\"$\\sigma(x)$\", r\"$\\sigma'(x)$\"])\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx2pRNyoXY-U"
      },
      "source": [
        "As you can see, the Sigmoid function saturates for very small and very large input values and as a result its derivative becomes very small, causing learning to slow down. In the next assignment we will learn how to circumvent this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDE8wsMIcHCN"
      },
      "source": [
        "## A2.6 Softmax and Cross Entropy Loss\n",
        "\n",
        "So far the classifier scores predicted by our network (also called 'logits') were unbounded and we simply trained the network to minimize the mean squared error between the scores and the one-hot encoded labels. However, in a classification task we would like to interpret the output of our network as class probabilities, i.e. for all inputs $x$ and all classes $k$ we should have $$0 \\leq P(Y=k|X=x) \\leq 1, \\qquad \\sum_kP(Y=k|X=x) = 1.$$\n",
        "This can be achieved by normalizing the logits $z$ using the Softmax layer:\n",
        "$$P(Y=k|X=x) = \\text{softmax}(\\mathbf{z})_k = \\frac{\\exp{z_k}}{\\sum_i^K \\exp{z_i}},$$\n",
        "where $K$ is the number of classes.\n",
        "\n",
        "Implement Softmax below and run the example. Verify that indeed the smallest and largest logits get assigned the smallest and largest probabilities and that the outputs are equal to the PyTorch implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjwd9ZGNgtEv"
      },
      "source": [
        "def Softmax(z):\n",
        "    \"\"\"\n",
        "    Computes softmax output for each sample in batch.\n",
        "\n",
        "    Args:\n",
        "      z: Tensor of logits, dimension [batch, classes].\n",
        "\n",
        "    Return:\n",
        "      p: Softmax probability distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    ########################################################################\n",
        "    #                  TODO: Implement Softmax function.                   #\n",
        "    ########################################################################\n",
        "\n",
        "    pass\n",
        "\n",
        "    ########################################################################\n",
        "    #                          END OF YOUR CODE                            #\n",
        "    ########################################################################\n",
        "\n",
        "    return p\n",
        "\n",
        "# Generate random logits, dimension [batch, classes]\n",
        "z = torch.randn(1,5)\n",
        "# Calculate softmax\n",
        "p = Softmax(z)\n",
        "\n",
        "# Verify against PyTorch implementation\n",
        "p_torch = torch.nn.functional.softmax(z, 1)\n",
        "print('Softmax implementation correct: ', torch.allclose(p, p_torch))\n",
        "\n",
        "# Plot z and p\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(r'Logits $z$')\n",
        "_=plt.bar(torch.arange(1,6), z[0,:])\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(r'Probabilities $p$')\n",
        "_=plt.bar(torch.arange(1,6), p[0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cthwWyegrsl"
      },
      "source": [
        "The Softmax layer is a vector extension of the Sigmoid function and so unfortunately comes with the same saturation problem causing learning to slow down. Luckily we can overcome this by replacing the MSE loss by the Cross Entropy (CE) loss:\n",
        "$$H(y,p) = -\\sum_i^K y_i \\log(p_i),$$\n",
        "where $y$ is the one-hot encoded label and $p$ is the class probability vector from the Softmax layer.\n",
        "Using the CE loss with the Softmax layer is effective as the $\\log$ in the CE loss can undo the $\\exp$ in the Softmax layer:\n",
        "$$\\log \\text{softmax}(\\mathbf{z})_k = z_k - \\log \\sum_i^K \\exp z_i.$$\n",
        "Because now $z_k$ has a direct contribution to the loss it can never saturate and the gradients will never become too small.\n",
        "\n",
        "The Softmax layer and Cross Entropy loss are used together so often that they are implemented as a single function in Pytorch as `torch.nn.CrossEntropyLoss`. Below we have implemented the CE loss with Softmax so we can use it to train our network. Feel free to go over the code, but you do not need to understand it exactly as the implementation is not very straightforward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEZF9yQ8cT01"
      },
      "source": [
        "def CrossEntropyLoss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes softmax output and cross-entropy loss.\n",
        "\n",
        "    Args:\n",
        "      y_true: Tensor containing true labels.\n",
        "      y_pred: Tensor containing predictions.\n",
        "\n",
        "    Return:\n",
        "      loss: Cross-entropy loss.\n",
        "      dy_pred: Gradient of loss w.r.t. y_pred.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate softmax using previously defined function\n",
        "    softmax = Softmax(y_pred)\n",
        "\n",
        "    # Convert one-hot vector to class id\n",
        "    y_true = torch.argmax(y_true, axis=1)\n",
        "    # Get number of samples in batch\n",
        "    n = y_true.shape[0]\n",
        "    # Calculate cross entropy loss between y_true and y_pred\n",
        "    log_likelihood = -torch.log(softmax[torch.arange(n),y_true])\n",
        "    # Average over all samples\n",
        "    loss = torch.mean(log_likelihood)\n",
        "\n",
        "    # Caculate the gradient\n",
        "    grad = softmax\n",
        "    softmax[torch.arange(n), y_true] -= 1\n",
        "    grad /= n\n",
        "\n",
        "    return loss, grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUlGT8Z7A4PW"
      },
      "source": [
        "We will now solve the XOR problem by training the network with the Cross Entropy loss and Sigmoid activation function. The network should be able to converge much faster than using the MSE loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K3xJzgrcQc6"
      },
      "source": [
        "# Define network dimensions\n",
        "in_features, hidden_dim, out_features = 2, 10, 2\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 5e-1  # step size for gradient descent\n",
        "optim_steps = 150  # network should generally converge within 100 steps\n",
        "\n",
        "# Define and initialize layers\n",
        "layers = [Linear(in_features, hidden_dim),\n",
        "          Sigmoid(),\n",
        "          Linear(hidden_dim, out_features)]\n",
        "\n",
        "# Initialize network\n",
        "net = Net(layers)\n",
        "\n",
        "# Define list to store loss and accuracy of each iteration\n",
        "losses = []\n",
        "accs = []\n",
        "\n",
        "for i in range(optim_steps):\n",
        "    # Perform forward pass with x_xor as input and y_pred as output variables\n",
        "    y_pred = net.forward(x_xor)\n",
        "\n",
        "    ########################################################################\n",
        "    #      TODO: Calculate Cross Entropy loss between prediction and       #\n",
        "    #                  labels (y_xor) and append to list.                  #\n",
        "    ########################################################################\n",
        "\n",
        "    pass\n",
        "\n",
        "    ########################################################################\n",
        "    #                          END OF YOUR CODE                            #\n",
        "    ########################################################################\n",
        "\n",
        "    ########################################################################\n",
        "    #            TODO: Perform backward pass and optimizer step.           #\n",
        "    ########################################################################\n",
        "\n",
        "    pass\n",
        "\n",
        "    ########################################################################\n",
        "    #                          END OF YOUR CODE                            #\n",
        "    ########################################################################\n",
        "\n",
        "    # Calculate accuracy of prediction\n",
        "    correct = torch.argmax(y_pred, axis=1) == torch.argmax(y_xor, axis=1)\n",
        "    accs.append(torch.sum(correct)/len(y_pred))\n",
        "\n",
        "# Print prediction\n",
        "print(y_pred)\n",
        "\n",
        "# Plot loss and accuracy\n",
        "plt.figure(figsize=(14,4))\n",
        "plt.subplot(1,3,1)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(losses)\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.plot(accs)\n",
        "plt.grid()\n",
        "\n",
        "# Show decision boundary\n",
        "plt.subplot(1,3,3)\n",
        "plot(x_xor, y_xor, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7fATXbHjJe0"
      },
      "source": [
        "## [EXTRA] A2.7 MNIST handwritten digit classification\n",
        "In this extra exercise you will use your neural network to solve a real-world classification problem: classifying handwritten digits.\n",
        "\n",
        "The MNIST dataset consists of a training set of 60,000 and a test set of 10,000 samples of 28x28 pixels, which we will flatten to a 1D vector representation to feed into the neural network. In every training iteration we will sample a random batch of 64 data samples (without replacement) to calculate the loss, gradients and update the weights. This random sampling is where the 'Stochastic' in Stochastic Gradient Descent comes from. After every epoch, i.e. after having processed all sampels in the training set, we will evaluate the model on the test set.\n",
        "\n",
        "The following code block downloads the dataset and visualizes some samples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4mMxSNwjHKw"
      },
      "source": [
        "# Preprocessing data: convert to tensors and normalize by subtracting dataset\n",
        "# mean and dividing by std.\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "# Get data from torchvision.datasets\n",
        "train_data = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Define data loaders used to iterate through dataset\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=1000)\n",
        "\n",
        "# Show some example images\n",
        "fig, axs = plt.subplots(5, 5, figsize=(5, 5))\n",
        "for i in range(25):\n",
        "    x, _ = test_data[i]\n",
        "    ax = axs[i // 5][i % 5]\n",
        "    ax.imshow(x.view(28, 28), cmap='gray')\n",
        "    ax.axis('off')\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbTGMd7kkcy-"
      },
      "source": [
        "########################################################################\n",
        "#             TODO: Define appropriate network dimensions.             #\n",
        "########################################################################\n",
        "\n",
        "pass\n",
        "\n",
        "########################################################################\n",
        "#                          END OF YOUR CODE                            #\n",
        "########################################################################\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 5e-1  # step size for gradient descent\n",
        "epochs = 10  # how many times to iterate through the intire training set\n",
        "\n",
        "# Define and initialize layers\n",
        "layers = [Linear(in_features, hidden_dim),\n",
        "          Sigmoid(),\n",
        "          Linear(hidden_dim, out_features)]\n",
        "\n",
        "# Initialize network\n",
        "net = Net(layers)\n",
        "\n",
        "# Define list to store loss of each iteration\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training loop\n",
        "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "        # Flatten input to 1D tensor\n",
        "        x_batch = x_batch.flatten(start_dim=1)\n",
        "        # Convert labels to one-hot encoding\n",
        "        y_batch = nn.functional.one_hot(y_batch, num_classes=10)\n",
        "\n",
        "        # Perform forward pass\n",
        "        y_pred = net.forward(x_batch)\n",
        "\n",
        "        ########################################################################\n",
        "        #      TODO: Calculate Cross Entropy loss between prediction and       #\n",
        "        #                     labels and append to list.                       #\n",
        "        ########################################################################\n",
        "\n",
        "        pass\n",
        "\n",
        "        ########################################################################\n",
        "        #                          END OF YOUR CODE                            #\n",
        "        ########################################################################\n",
        "\n",
        "\n",
        "        ########################################################################\n",
        "        #            TODO: Perform backward pass and optimizer step.           #\n",
        "        ########################################################################\n",
        "\n",
        "        pass\n",
        "\n",
        "        ########################################################################\n",
        "        #                          END OF YOUR CODE                            #\n",
        "        ########################################################################\n",
        "\n",
        "        # Calculate accuracy of prediction\n",
        "        correct = torch.argmax(y_pred, axis=1) == torch.argmax(y_batch, axis=1)\n",
        "        train_accs.append(torch.sum(correct)/len(y_pred))\n",
        "\n",
        "        # Print progress\n",
        "        if i % 200 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch + 1, i * len(x_batch), len(train_loader.dataset),\n",
        "                100. * i / len(train_loader), loss))\n",
        "\n",
        "    # Validation loop\n",
        "    test_loss = 0\n",
        "    total_correct = 0\n",
        "    for x_batch, y_batch in test_loader:\n",
        "        # Flatten input to 1D tensor\n",
        "        x_batch = x_batch.flatten(start_dim=1)\n",
        "        # Convert labels to one-hot encoding\n",
        "        y_batch = nn.functional.one_hot(y_batch, num_classes=10)\n",
        "\n",
        "        # Perform forward pass with x_xor as input and y_pred as output variables\n",
        "        y_pred = net.forward(x_batch)\n",
        "\n",
        "        ########################################################################\n",
        "        #   TODO: Calculate Cross Entropy loss between prediction and labels.  #\n",
        "        ########################################################################\n",
        "\n",
        "        pass\n",
        "\n",
        "        ########################################################################\n",
        "        #                          END OF YOUR CODE                            #\n",
        "        ########################################################################\n",
        "\n",
        "        # Keep track of total loss over test set\n",
        "        test_loss += loss\n",
        "\n",
        "        # Calculate accuracy of prediction\n",
        "        correct = torch.argmax(y_pred, axis=1) == torch.argmax(y_batch, axis=1)\n",
        "        total_correct += torch.sum(correct)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = 100. * total_correct / len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, total_correct, len(test_loader.dataset), test_acc))\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(train_losses)\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.plot(train_accs)\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48obmfkSS956"
      },
      "source": [
        "Your network can now classify handwritten digits with reasonably high accuracy. Well done!\n",
        "\n",
        "This concludes the second assignment. From the next assignment onwards we will start using more and more built-in PyTorch functionality and will get to know some more advanced methods for training neural networks. Also, we will implement the most prominent building block of modern computer vision: the convolutional layer."
      ]
    }
  ]
}